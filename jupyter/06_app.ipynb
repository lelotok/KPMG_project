{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different aproaches to use in django app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "import string\n",
    "import unidecode\n",
    "\n",
    "\n",
    "import transformers\n",
    "import spacy\n",
    "\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE='2022-06-09'\n",
    "\n",
    "MAINLINK='https://www.ejustice.just.fgov.be/cgi/summary_body.pl?language=nl&pub_date='\n",
    "DETAILLINK='https://www.ejustice.just.fgov.be/cgi/article_body.pl?language=nl&caller=summary&pub_date='\n",
    "\n",
    "\n",
    "\n",
    "res = requests.get(MAINLINK + DATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_numac_numbers(res:str):\n",
    "    _numacs=[]\n",
    "\n",
    "    soup = BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        value = soup.find_all('input', {'name': 'numac'})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Got unhandled exception %s\" % str(e))\n",
    "\n",
    "    for v in value:\n",
    "        _numacs.append(v['value'].strip())\n",
    "    return _numacs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numac_links(_numacs:list):\n",
    "    links=[]\n",
    "\n",
    "    for _a in _numacs:\n",
    "        link = f\"{DETAILLINK}{DATE}&numac={_a}\"\n",
    "        links.append(link)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numacs=get_numac_numbers(res.text)\n",
    "numac_links=create_numac_links(numacs)\n",
    "numac_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the scraped text\n",
    "def clean(_a:str):\n",
    "    d=re.sub(r'(?<=[.,;,:])(?=[^\\s])', r' ', _a)\n",
    "    \n",
    "    document_test= unidecode.unidecode(d)\n",
    "    document_test = document_test.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n",
    "\n",
    "    pattern = re.compile(r'\\s+') \n",
    "    Without_whitespace = re.sub(pattern, ' ', document_test)\n",
    "    # There are some instances where there is no space after '?' & ')', \n",
    "    # I am replacing these with one space so that It will not consider two words as one token.\n",
    "    document_test = Without_whitespace.replace('?', ' ? ').replace(')', ') ')\n",
    "    \n",
    " \n",
    "    document_test = re.sub(r\"[^a-zA-Z0-9:$-,%.?!]+\", ' ', document_test) \n",
    "   \n",
    "    # Remove Mentions\n",
    "    document_test = re.sub(r'@\\w+', '', document_test)\n",
    "\n",
    "    #Not necessary; \n",
    "    #document_test = re.sub(r\"[^a-zA-Z:$-,%.?!]+\", ' ', document_test)\n",
    "\n",
    "    return document_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape the article \n",
    "\n",
    "def scrape_numac(_numac_links:list):\n",
    "    _count=0 # to check which line\n",
    "    nl_list=[]\n",
    "    for a in _numac_links:\n",
    "        _count+=1\n",
    "        res = requests.get(a)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        for sup in soup.find_all('sup'):\n",
    "            sup.unwrap()\n",
    "    \n",
    "        \n",
    "        text=soup.text\n",
    "        text=text.replace('\\n',\"\")\n",
    "        lst=text.split('Numac :')[1].split(text.split('Numac :')[2])\n",
    "    \n",
    "\n",
    "        article=lst[1].split('begin eerste woord laatste')[0].strip()\n",
    "        article=clean(article)\n",
    "        nl_list.append(article)\n",
    "        print(_count)\n",
    "    return nl_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_list=scrape_numac(numac_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUMMARY\n",
    "from transformers import MBartConfig\n",
    "\n",
    "\n",
    "undisputed_best_model = transformers.MBartForConditionalGeneration.from_pretrained(\n",
    "    \"ml6team/mbart-large-cc25-cnn-dailymail-nl-finetune\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = transformers.MBartTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "summarization_pipeline = transformers.pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=undisputed_best_model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "summarization_pipeline.model.config.decoder_start_token_id = tokenizer.lang_code_to_id[\n",
    "    \"nl_XX\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Creating the summary with pretrained model and saving to a csv file in case an error and appending to dataframe \n",
    "\n",
    "def summarise(_nl_list:list):\n",
    "    counter=0\n",
    "    summary=[]\n",
    "    for text in _nl_list:\n",
    "        \n",
    "\n",
    "        t= summarization_pipeline(\n",
    "            text,\n",
    "            do_sample=True,\n",
    "            top_p=0.75,\n",
    "            top_k=50,\n",
    "            num_beams=4,\n",
    "            min_length=50,\n",
    "            early_stopping=True,\n",
    "            truncation=True,\n",
    "        )[0][\"summary_text\"]\n",
    "\n",
    "        summary.append(t)\n",
    "\n",
    "        counter=counter+1\n",
    "        print(counter)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary=summarise(nl_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMILARITY=0.80\n",
    "tags='aanslagjaar arbeidsongeschiktheidsuitkeringen bedrijfsinkomsten bedrijfskosten bedrijfstoeslag bedrijfsvoorheffing belasting belastingverdragen belastingverhoging'\\\n",
    "     'belastingvermindering belastingvoet belastingvoordeel belastingvrije beroepsinkomsten beroepskosten bezoldiging btw derdebetalersregeling dienstverplichtingen erfbelasting'\\\n",
    "     'financieringskosten heffing inkomsten inkomstenderving investeringsaftrek kapitaalaflossingen kapitaalvermindering kostenvermindering omzetbelasting personenbelasting'\\\n",
    "     'prestatievergoeding rechtspersonenbelasting registratierechten schenkbelasting socialezekerheidsbijdragen solidariteitsbijdrage uitbetalingsinstelling vennootschapsbelasting'\\\n",
    "     'verminderingen vervangingsinkomsten voorafbetalingen voorbelasting voorheffing vrijstellingsregeling waardevermindering werkgeversbijdrage werkingskosten zekerheidsbijdragen'\\\n",
    "     'invaliditeit verzekering'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tags=tags.split(' ')\n",
    "\n",
    "real_tag_tensors=[]\n",
    "for a in _tags:\n",
    "    input_ids = torch.tensor(tokenizer.encode(a)).unsqueeze(0)  # Batch size 1\n",
    "    outputs = undisputed_best_model(input_ids)\n",
    "    real_tag_tensors.append(outputs.logits)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode('In de periode tussen 7 en 13 juni zijn dagelijks gemiddeld 60,7 mensen in het ziekenhuis opgenomen omwille van een coronabesmetting, een stijging met 15 procent in vergelijking met de voorgaande periode. Dat blijkt uit recente cijfers van gezondheidsinstituut Sciensano. Ook het aantal besmettingen stijgt.')).unsqueeze(0)  # Batch size 1\n",
    "outputs = undisputed_best_model(input_ids)\n",
    "last_hidden_states = outputs[2]  # The last hidden-state is the first element of the output tuple\n",
    "\n",
    "input_ids2 = torch.tensor(tokenizer.encode('In de periode tussen 7 en 13 juni zijn dagelijks gemiddeld 60,7 mensen in het ziekenhuis opgenomen omwille van een coronabesmetting, een stijging met 15 procent in vergelijking met de voorgaande periode. Dat blijkt uit recente cijfers van gezondheidsinstituut Sciensano. Ook het aantal besmettingen stijgt.')).unsqueeze(0)  # Batch size 1\n",
    "outputs2 = undisputed_best_model(input_ids2)\n",
    "last_hidden_states2 = outputs2[2]  # The last hidden-state is the first element of the output tuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3074,  0.2814,  0.6020,  ..., -0.0595, -0.1762,  0.0880],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "torch.Size([1024])\n",
      "tensor([ 0.3074,  0.2814,  0.6020,  ..., -0.0595, -0.1762,  0.0880],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "sentence_embedding = torch.mean(outputs[-1], dim=1).squeeze()\n",
    "print(sentence_embedding)\n",
    "print(sentence_embedding.size())\n",
    "sentence_embedding2 = torch.mean(outputs2[-1], dim=1).squeeze()\n",
    "print(sentence_embedding2)\n",
    "print(sentence_embedding2.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "q=cos(sentence_embedding, sentence_embedding2)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"nl_core_news_lg\")\n",
    "real_tags=nlp(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagging(real_tags,summary_tags):\n",
    "    summary_tag_list={}\n",
    "\n",
    "    for _a in summary_tags:\n",
    "        \n",
    "        for token in real_tags:\n",
    "            q=round(token.similarity(_a),3)\n",
    "        \n",
    "            if q > SIMILARITY:\n",
    "                \n",
    "                #add token to dict\n",
    "                summary_tag_list[token]=q\n",
    "                #print(_a,token,_a.similarity(token))\n",
    "    return summary_tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=[]\n",
    "t=0\n",
    "for a in summary:\n",
    "    print(t)\n",
    "    text=a.lower()\n",
    "    summary_tags=nlp(text)      \n",
    "    \n",
    "    summary_tag_list=tagging(real_tags,summary_tags)\n",
    "    dict1 = summary_tag_list\n",
    "    sorted_dict = {}\n",
    "    sorted_keys = sorted(dict1, key=dict1.get,reverse=True)  # [1, 3, 2]\n",
    "\n",
    "    for w in sorted_keys:\n",
    "        sorted_dict[w] = dict1[w]\n",
    "\n",
    "    first5pairs = {k: sorted_dict[k] for k in list(sorted_dict)[:5]}\n",
    "\n",
    "\n",
    "    keys.append(first5pairs)\n",
    "    t=t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    {'date':DATE,\n",
    "    'numac':numacs,\n",
    "    'nltext':nl_list,\n",
    "        'nllink':numac_links,\n",
    "        'summary': summary,\n",
    "     'nltags': keys\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data['nltags']=data['nltags'].astype(str)\n",
    "data['nltags']=data[\"nltags\"].str.strip('{}')\n",
    "data['nltags'] = data['nltags'].replace('',np.nan,regex = True)\n",
    "data.dropna(subset = [\"nltags\"], inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from btax.taxtag.models import Article\n",
    "for index, row in data.iterrows():\n",
    "    print(row['date'], row['numac'])\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# class Article(models.Model):\n",
    "#     date = models.DateField()\n",
    "#     numac = models.CharField(max_length=15)\n",
    "#     link=models.CharField(max_length=150)\n",
    "#     nl_text=models.TextField()\n",
    "#     nl_sum=models.TextField()\n",
    "#     nl_tags=models.TextField()\n",
    "#     created_at = models.DateTimeField(auto_now_add=True)\n",
    "#     updated_at = models.DateTimeField(auto_now=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data['nltags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all=list(zip(keys,summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78048d5901510aba3f63ef87ca1b57ab9d805b11f89c30917e9241a8225bb540"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venvkpmg': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
